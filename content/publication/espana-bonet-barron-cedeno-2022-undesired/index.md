---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: The (Undesired) Attenuation of Human Biases by Multilinguality
subtitle: ''
summary: ''
authors:
- Cristina España-Bonet
- Alberto Barrón-Cedeño
tags: []
categories: []
date: '2022-12-01'
lastmod: 2023-11-02T13:06:35+01:00
featured: true
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2023-11-02T12:06:35.014320Z'
publication_types:
- '1'
abstract: Some human preferences are universal. The odor of vanilla is perceived as
  pleasant all around the world. We expect neural models trained on human texts to
  exhibit these kind of preferences, i.e. biases, but we show that this is not always
  the case. We explore 16 static and contextual embedding models in 9 languages and,
  when possible, compare them under similar training conditions. We introduce and
  release CA-WEAT, multilingual cultural aware tests to quantify biases, and compare
  them to previous English-centric tests. Our experiments confirm that monolingual
  static embeddings do exhibit human biases, but values differ across languages, being
  far from universal. Biases are less evident in contextual models, to the point that
  the original human association might be reversed. Multilinguality proves to be another
  variable that attenuates and even reverses the effect of the bias, specially in
  contextual multilingual models. In order to explain this variance among models and
  languages, we examine the effect of asymmetries in the training corpus, departures
  from isomorphism in multilingual embedding spaces and discrepancies in the testing
  measures between languages.
publication: '*Proceedings of the 2022 Conference on Empirical Methods in Natural
  Language Processing*'
doi: 10.18653/v1/2022.emnlp-main.133
links:
- name: URL
  url: https://aclanthology.org/2022.emnlp-main.133
---
