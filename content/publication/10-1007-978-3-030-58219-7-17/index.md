---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: 'Overview of CheckThat! 2020: Automatic Identification and Verification of
  Claims in Social Media'
subtitle: ''
summary: ''
authors:
- Alberto Barrón-Cedeño
- Tamer Elsayed
- Preslav Nakov
- Giovanni Da San Martino
- Maram Hasanain
- Reem Suwaileh
- Fatima Haouari
- Nikolay Babulkov
- Bayan Hamdan
- Alex Nikolov
- Shaden Shaar
- Zien Sheikh Ali
tags: []
categories: []
date: '2020-01-01'
lastmod: 2022-01-27T17:35:00+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-01-27T16:35:00.712006Z'
publication_types:
- '1'
abstract: 'We present an overview of the third edition of the CheckThat!  Lab at CLEF
  2020. The lab featured five tasks in two different languages: English and Arabic.
  The first four tasks compose the full pipeline of claim verification in social media:
  Task 1 on check-worthiness estimation, Task 2 on retrieving previously fact-checked
  claims, Task 3 on evidence retrieval, and Task 4 on claim verification. The lab
  is completed with Task 5 on check-worthiness estimation in political debates and
  speeches. A total of 67 teams registered to participate in the lab (up from 47 at
  CLEF 2019), and 23 of them actually submitted runs (compared to 14 at CLEF 2019).
  Most teams used deep neural networks based on BERT, LSTMs, or CNNs, and achieved
  sizable improvements over the baselines on all tasks. Here we describe the tasks
  setup, the evaluation results, and a summary of the approaches used by the participants,
  and we discuss some lessons learned. Last but not least, we release to the research
  community all datasets from the lab as well as the evaluation scripts, which should
  enable further research in the important tasks of check-worthiness estimation and
  automatic claim verification.'
publication: '*Experimental IR Meets Multilinguality, Multimodality, and Interaction*'
---
