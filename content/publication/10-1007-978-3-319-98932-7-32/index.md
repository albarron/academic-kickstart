---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Overview of the CLEF-2018 CheckThat! Lab on Automatic Identification and Verification
  of Political Claims
subtitle: ''
summary: ''
authors:
- Preslav Nakov
- Alberto Barrón-Cedeño
- Tamer Elsayed
- Reem Suwaileh
- Lluís Màrquez
- Wajdi Zaghouani
- Pepa Atanasova
- Spas Kyuchukov
- Giovanni Da San Martino
tags: []
categories: []
date: '2018-01-01'
lastmod: 2022-01-27T17:35:04+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2022-01-27T18:00:12.843804Z'
publication_types:
- '1'
abstract: We present an overview of the CLEF-2018 CheckThat! Lab on Automatic Identification
  and Verification of Political Claims. In its starting year, the lab featured two
  tasks. Task 1 asked to predict which (potential) claims in a political debate should
  be prioritized for fact-checking; in particular, given a debate or a political speech,
  the goal was to produce a ranked list of its sentences based on their worthiness
  for fact-checking. Task 2 asked to assess whether a given check-worthy claim made
  by a politician in the context of a debate/speech is factually true, half-true,
  or false. We offered both tasks in English and in Arabic. In terms of data, for
  both tasks, we focused on debates from the 2016 US Presidential Campaign, as well
  as on some speeches during and after the campaign (we also provided translations
  in Arabic), and we relied on comments and factuality judgments from factcheck.organd
  snopes.com, which we further refined manually. A total of 30 teams registered to
  participate in the lab, and 9 of them actually submitted runs. The evaluation results
  show that the most successful approaches used various neural networks (esp. for
  Task 1) and evidence retrieval from the Web (esp. for Task 2). We release all datasets,
  the evaluation scripts, and the submissions by the participants, which should enable
  further research in both check-worthiness estimation and automatic claim verification.
publication: '*Experimental IR Meets Multilinguality, Multimodality, and Interaction*'
---
