{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the preprocessing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"The input text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The first time you use the stopwords, you have to download them!\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the \"corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "sentence = sentences.lower()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words representation\n",
    "\n",
    "Let us compute the BoW representation for our toy corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpus into a dictionary\n",
    "corpus ={}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    sentence = sent.lower()                 # Case folding\n",
    "    tokens = tokenizer.tokenize(sentence)   # Tokenisation \n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
    "         stems)\n",
    "\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading it into a pandas dataframe\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "\n",
    "#df[df.columns[:10]]\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dot product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([2, 4, 6])\n",
    "\n",
    "sum_dot = 0\n",
    "\n",
    "for i in range(len(v1)):\n",
    "    sum_dot += v1[i] * v2[i]\n",
    "    print(sum_dot)\n",
    "print(\"Result\", sum_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Note that we are vectorizing the multiplication of the two vectors\n",
    "dot = (v1 * v2).sum()\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product can be used to measure the overlapping between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to compute the transpose of the matrix \n",
    "df = df.T\n",
    "\n",
    "#How can I print it?\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do these numbers come from?\n",
    "print(sentences)\n",
    "[(k, v) for (k, v) in (df.sent0 & df.sent3).items() if v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is your first **vector space model**!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "# Let us have a look at the lexicon\n",
    "#sa.lexicon\n",
    "[(tok, score) for tok, score in sa.lexicon.items() if tok.startswith(\"c\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us see if there are bigrams\n",
    "[(tok, score) for tok, score in sa.lexicon.items() if \" \" in tok]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's score!!\n",
    "sa.polarity_scores(text=\"Python is very readable and it's great for NLP.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.polarity_scores(text=\"Python is not a bad choice for most applications.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Absolutely perfect! Love it! :-) :-) :-)\",\n",
    "          \"Horrible! Completely useless. :(\",\n",
    "           \"It was OK. Some good and some bad things.\"]\n",
    "\n",
    "for doc in corpus:\n",
    "    scores = sa.polarity_scores(doc)\n",
    "    print('{:+}: {}'.format(scores['compound'], doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the Amazon review\n",
    "\n",
    "text = \"\"\"\"This monitor is de\f",
    "nitely a good value. Does it have superb color and \n",
    "contrast? No. Does it boast the best refresh rate on the market? No. \n",
    "But if you're tight on money, this thing looks and preforms great for the money. \n",
    "It has a Matte screen which does a great job at eliminating glare. The chassis it's enclosed \n",
    "within is absolutely stunning.\")\"\"\"\n",
    "len(text.split())\n",
    "\n",
    "for i in [10, 20, 45, 60]:\n",
    "    t = \" \".join(text.split()[:i])\n",
    "    print(i,\"\\t\", t)\n",
    "    print(\"ONE TIME\", sa.polarity_scores(t))\n",
    "    print(\"THREE TIME\", sa.polarity_scores(\" \".join([t, t, t])))\n",
    "    print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sa.polarity_scores(\"this is not good\"))\n",
    "print(sa.polarity_scores(\"this is not good at all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the tweet\n",
    "sa.polarity_scores(\"His ass didnt concede until July 12, 2016. Because he was throwing a tantrum. I can't say this enough: Fuck Bernie Sanders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
