{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From BoW to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from nlpia.loaders import clean_columns\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The difference between a binary and a _counting_ BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the vocabulary \n",
    "set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the tokens in the sentence\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the tf for a specific word in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)         # what is this doing?\n",
    "\n",
    "print(\"Times harry appears:\", times_harry_appears)\n",
    "print(\"Size of the vocabulary:\", num_unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising $tf$: dividing by the total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = times_harry_appears / num_unique_words\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: I have usually seen \n",
    "\n",
    "$\\sum_w freq(w)$ \n",
    "\n",
    "as denominator. That is, sum of all the frequencies. \n",
    "I'm sticking to the book, which uses length of the vocabulary instead.\n",
    "\n",
    "Back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $tf$ on a slightly longer text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikipedia article on COVID as (roughly) one year ago\n",
    "txt = \"\"\"The Russo-Ukrainian War is an ongoing war primarily \n",
    "involving Russia, pro-Russian forces, and Belarus on one side, \n",
    "and Ukraine and its international supporters on the other. \n",
    "Conflict began in February 2014 following the Revolution of \n",
    "Dignity, and focused on the status of Crimea and parts of the \n",
    "Donbas, internationally recognised as part of Ukraine. The \n",
    "conflict includes the Russian annexation of Crimea (2014), \n",
    "the war in Donbas (2014–present), naval incidents, cyberwarfare, \n",
    "and political tensions. Intentionally concealing its involvement, \n",
    "Russia gave military backing to separatists in the Donbas from \n",
    "2014 onwards. Having built up a large military presence on the \n",
    "border from late 2021, Russia launched a full-scale invasion of \n",
    "Ukraine on 24 February 2022, which is ongoing.\n",
    "\n",
    "Following the Euromaidan protests and a revolution resulting in \n",
    "the removal of pro-Russian President Viktor Yanukovych on 22 \n",
    "February 2014, pro-Russian unrest erupted in parts of Ukraine. \n",
    "Russian soldiers without insignia took control of strategic \n",
    "positions and infrastructure in the Ukrainian territory of \n",
    "Crimea. Unmarked Russian troops seized the Crimean Parliament \n",
    "and Russia organized a widely-criticised referendum, whose \n",
    "outcome was for Crimea to join Russia. It then annexed Crimea. \n",
    "In April 2014, demonstrations by pro-Russian groups in the \n",
    "Donbas region of Ukraine escalated into a war between the \n",
    "Ukrainian military and Russian-backed separatists of the \n",
    "self-declared Donetsk and Luhansk republics.\n",
    "\n",
    "In August 2014, unmarked Russian military vehicles crossed the \n",
    "border into the Donetsk republic. An undeclared war began between \n",
    "Ukrainian forces and separatists intermingled with Russian troops, \n",
    "although Russia denied the presence of its troops in the Donbas. \n",
    "The war settled into a stalemate, with repeated failed attempts \n",
    "at ceasefire. In 2015, a package of agreements called Minsk II \n",
    "were signed by Russia and Ukraine, but a number of disputes \n",
    "prevented them from being fully implemented. By 2019, 7% of \n",
    "Ukraine's territory was classified by the Ukrainian government \n",
    "as temporarily occupied territories, while the Russian government \n",
    "had indirectly acknowledged the presence of its troops in Ukraine.\n",
    "\n",
    "In 2021 and early 2022, there was a major Russian military \n",
    "build-up around Ukraine's borders. NATO accused Russia of planning \n",
    "an invasion, which it denied. Russian President Vladimir Putin \n",
    "criticized the enlargement of NATO as a threat to his country and \n",
    "demanded Ukraine be barred from ever joining the military \n",
    "alliance. He also expressed Russian irredentist views, questioned \n",
    "Ukraine's right to exist, and stated Ukraine was wrongfully \n",
    "created by Soviet Russia. On 21 February 2022, Russia officially \n",
    "recognised the two self-proclaimed separatist states in the Donbas, \n",
    "and sent troops to the territories. Three days later, Russia \n",
    "invaded Ukraine after Putin announced a \"special military \n",
    "operation\". Much of the international community and organizations \n",
    "such as Amnesty International have condemned Russia for its actions \n",
    "in post-revolutionary Ukraine, accusing it of breaking \n",
    "international law and violating Ukrainian sovereignty. Many \n",
    "countries implemented economic sanctions against Russia, Russian \n",
    "individuals, or companies, especially after the 2022 invasion. \n",
    "\"\"\"\n",
    "\n",
    "kk = \"\"\"Coronavirus disease 2019 (COVID-19) is an infectious disease caused by \n",
    "severe acute respiratory syndrome coronavirus 2 (SARS coronavirus 2, \n",
    "or SARS-CoV-2), a virus closely related to the SARS virus. The disease \n",
    "was discovered and named during the 2019–20 coronavirus outbreak. \n",
    "Those affected may develop a fever, dry cough, fatigue, and shortness \n",
    "of breath. A sore throat, runny nose or sneezing is less common. While \n",
    "the majority of cases result in mild symptoms, some can progress to \n",
    "pneumonia and multi-organ failure.\n",
    "\n",
    "The infection is spread from one person to others via respiratory droplets \n",
    "produced from the airways, often during coughing or sneezing. Time from exposure \n",
    "to onset of symptoms is generally between 2 and 14 days, with an average of 5 \n",
    "days. The standard method of diagnosis is by reverse transcription polymerase \n",
    "chain reaction (rRT-PCR) from a nasopharyngeal swab or sputum sample, with \n",
    "results within a few hours to 2 days. Antibody assays can also be used, using a \n",
    "blood serum sample, with results within a few days. The infection can also be \n",
    "diagnosed from a combination of symptoms, risk factors and a chest CT scan \n",
    "showing features of pneumonia.\n",
    "\n",
    "Correct handwashing technique, maintaining distance from people who are coughing \n",
    "and not touching one's face with unwashed hands are measures recommended to \n",
    "prevent the disease. It is also recommended to cover one's nose and mouth with a \n",
    "tissue or a bent elbow when coughing. Masks are also recommended for those who \n",
    "are taking care of someone with a suspected infection but not for the general \n",
    "public. There is no vaccine or specific antiviral treatment, with management \n",
    "involving treatment of symptoms, supportive care and experimental measures.\n",
    "The case fatality rate is estimated at between 1% and 3%.\n",
    "\n",
    "The World Health Organization (WHO) has declared the 2019–20 coronavirus \n",
    "outbreak a Public Health Emergency of International Concern (PHEIC). As of 7 \n",
    "March 2020, evidence of local transmission of the disease has been found in \n",
    "multiple countries across all six WHO regions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(txt.lower())\n",
    "token_counts = Counter(tokens)\n",
    "# display the most common tokens in the document\n",
    "token_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent words are (almost) all **stopwords**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only the first time\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "# pay attention to what is going on here (do we need to unfold it?)\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "counts = Counter(tokens)\n",
    "counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**: normalise these frequencies\n",
    "\n",
    "back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorising\n",
    "\n",
    "Not a **dictionary** of counts, but a **vector** of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in counts.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "    \n",
    "document_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "print(docs)\n",
    "print()\n",
    "print(\"\\n\".join(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the full lexicon\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    #doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "    doc_tokens.append(sorted(tokenizer.tokenize(doc.lower())))\n",
    "\n",
    "print(len(doc_tokens[0]))\n",
    "doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating all lists into one\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "print(len(all_doc_tokens))\n",
    "all_doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "print(len(lexicon))\n",
    "\n",
    "# Notice that usually the lexicon is called the set of types\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it is clear now: our vectors must have 18 values\n",
    "\n",
    "Creating the initial zero vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# Creating a template dictionary with all zeros (\"base vector\")\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Make copies of that base vector\n",
    "# 2. Update the values of the vector for each document\n",
    "# 3. Store them in an array\n",
    "#\n",
    "# We use copy.copy to avoid reference copies and do independent copies\n",
    "\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "    \n",
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity to compare two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    \"\"\"\n",
    "    cosine_sim computes the cosine similarity between two vectors\n",
    "    :param vec1: Dictionary with vector (Counter)\n",
    "    :param vec2: Dictionary with vector (Counter)\n",
    "    :return: cosine(vec1, vec2)\n",
    "    \"\"\"\n",
    "    \n",
    "    #Convert our dictionaries into lists for easier matching\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    # this parenthesis is important!\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"cosine(0, 1)=\", cosine_sim(doc_vectors[0], doc_vectors[1]))\n",
    "print(\"cosine(0, 2)=\", cosine_sim(doc_vectors[0], doc_vectors[2]))\n",
    "print(\"cosine(1, 2)=\", cosine_sim(doc_vectors[1], doc_vectors[2]))\n",
    "print(\"cosine(1, 1)=\", cosine_sim(doc_vectors[1], doc_vectors[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's Law\n",
    "\n",
    "Let us play with the Brown corpus to see Zipf's law. The [Brown corpus](https://en.wikipedia.org/wiki/Brown_Corpus) is one of the classical corpora in English, with 1M+ words, and a good alternative for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only the first time\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "# tokens\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including part of speech\n",
    "brown.tagged_words()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the Brown corpus (number of tokens)\n",
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a set of punctuation marks to filter them out\n",
    "puncts = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "\n",
    "word_list = (x.lower() for x in brown.words() if x not in puncts)\n",
    "token_counts = Counter(word_list)\n",
    "token_counts.most_common(20)\n",
    "\n",
    "#Does this follow Zipf's distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading two sections of document \"kite\" (from NLP in Action)\n",
    "from nlpia.data.loaders import kite_text, kite_history\n",
    "\n",
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "\n",
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "\n",
    "print(intro_tokens)\n",
    "print(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_total = len(intro_tokens)\n",
    "intro_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_total = len(history_tokens)\n",
    "history_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "\n",
    "history_counts = Counter(history_tokens)\n",
    "history_tf['kite'] = history_counts['kite'] / history_total\n",
    "\n",
    "'Term Frequency of \"kite\" in intro is: {:.4f}'.format(intro_tf['kite'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Term Frequency of \"kite\" in history is: {:.4f}'.format(history_tf['kite'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$freq(kite, intro) \\sim 2 * freq(kite, history)$\n",
    "\n",
    "Is intro more about kites than history?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giving perspective by looking at the frequency of other words\n",
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "print('Term Frequency of \"and\" in intro is: {:.4f}'.format(intro_tf['and']))\n",
    "print('Term Frequency of \"and\" in history is: {:.4f}'.format(history_tf['and']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$freq(and, \\cdot)$ is quite similar to $freq(kite, \\cdot)$\n",
    "\n",
    "So, the document is about **kites** and about **ands**?\n",
    "\n",
    "(back to the slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents with *\n",
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "        \n",
    "num_docs_containing_kite = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'kite' in doc:\n",
    "        num_docs_containing_kite += 1\n",
    "        \n",
    "num_docs_containing_china = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'china' in doc:\n",
    "        num_docs_containing_china += 1\n",
    "        \n",
    "print(\"and:\", num_docs_containing_and)\n",
    "print(\"kite:\", num_docs_containing_kite)\n",
    "print(\"china:\", num_docs_containing_china)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf(china)\n",
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total\n",
    "\n",
    "print(intro_tf)\n",
    "print(history_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idf for all 3\n",
    "num_docs = 2\n",
    "idf = {}\n",
    "idf['and'] = num_docs / num_docs_containing_and\n",
    "idf['kite'] = num_docs / num_docs_containing_kite\n",
    "idf['china'] = num_docs / num_docs_containing_china\n",
    "\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf for the intro\n",
    "\n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf for the history\n",
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * idf['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intro_tfidf)\n",
    "print(history_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching on \"Harry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not an extremely efficient implementation\n",
    "document_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)\n",
    "\n",
    "document_tfidf_vectors\n",
    "# Notice what happened to Harry!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How long does it take to get to the hairy store?\"\n",
    "query_vec = copy.copy(zero_vector)\n",
    "# query_vec = copy.copy(zero_vector)\n",
    "\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    " \n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in docs:\n",
    "      if key in _doc.lower():\n",
    "        docs_containing_key += 1\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(docs) / docs_containing_key\n",
    "    query_vec[key] = tf * idf\n",
    "query_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim(query_vec, document_tfidf_vectors[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs)\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn to build tf-idf matrices\n",
    "\n",
    "Now, rather than implementing everything ourselves, we will use a well-known python library to compute it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same small corpus as before\n",
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "corpus = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "\"\"\"min_df: ignore terms that have a document frequency \n",
    "strictly lower than the given threshold (aka cut-off).\n",
    "\"\"\"\n",
    "\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\"\"\"model is a sparse tf-idf matrix (mostly zeros) \n",
    "sklearn does not store zeros to save resources\"\"\"\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert it into a matrix in one line!\n",
    "print(\"\\n--\\n\".join(corpus))\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
