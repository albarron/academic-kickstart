{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the Google News embeddings (300d). **3GB+ of RAM are required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I WILL EXECUTE THIS ONE ONLY ONCE!\n",
    "GOOGLE_VECTORS = \"/Users/albarron/corpora/embeddings/GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "wv = KeyedVectors.load_word2vec_format(GOOGLE_VECTORS,\n",
    "    binary=True)# , limit=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Screening the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.Series(wv.vocab)\n",
    "print(vocab.iloc[0: 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Notice that in the book there is one 0 missing\n",
    "print(vocab.iloc[100000: 1000006])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab['New_York'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the distance between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the vectors (I don't really need these variables)\n",
    "v1 = wv['Illinois'] \n",
    "v2 = wv['Illini']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "np.linalg.norm(v1 - v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine \"distance\"\n",
    "cos_similarity = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "1 - cos_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to the slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=['city', 'cities', 'us'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance between cities\n",
    "\n",
    "1. Getting a list of cities from a fix list (Geocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a dataset from nlpia\n",
    "#from nlpia.data.loaders import get_data\n",
    "#cities = get_data('cities')\n",
    "# This downloader from nlpia doesn't work. \n",
    "# Download it from https://www.dropbox.com/s/tcri5eyzpabhnyy/cities.csv.gz?dl=1\n",
    "# and save it in the same place where your notebook is\n",
    "cities = pd.read_csv('cities.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.head(1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping US cities only\n",
    "us = cities[(cities.country_code == 'US') & (cities.admin1_code.notnull())].copy()\n",
    "\n",
    "# Loading states from another repo\n",
    "states = pd.read_csv('http://www.fonz.net/blog/wp-content/uploads/2008/04/states.csv')\n",
    "states = dict(zip(states.Abbreviation, states.State))\n",
    "\n",
    "# adding the info to \n",
    "us['city'] = us.name.copy()\n",
    "us['st'] = us.admin1_code.copy()\n",
    "us['state'] = us.st.map(states)\n",
    "us[us.columns[-3:]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are these cities in our vocabulary?\n",
    "\n",
    "vocab = pd.np.concatenate([us.city, us.st, us.state])\n",
    "vocab = np.array([word for word in vocab if word in wv.wv])\n",
    "vocab[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *Adding* state info to the game (as there are homonymns in different states)\n",
    "city_plus_state = []\n",
    "for c, state, st in zip(us.city, us.state, us.st):\n",
    "    if c not in vocab:\n",
    "        continue\n",
    "    row = []\n",
    "    if state in vocab:\n",
    "        row.extend(wv[c] + wv[state])\n",
    "    else:\n",
    "        row.extend(wv[c] + wv[st])\n",
    "    city_plus_state.append(row)\n",
    "us_300D = pd.DataFrame(city_plus_state)\n",
    "us_300D\n",
    "# back to the slides "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "# us_300D = get_data('cities_us_wordvectors')\n",
    "us_2D = pca.fit_transform(us_300D.iloc[:, :300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(us_2D))\n",
    "us_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "from nlpia.data.loaders import get_data\n",
    "df = get_data('cities_us_wordvectors_pca2_meta')\n",
    "\n",
    "# population to numbers\n",
    "df['population'] = pd.to_numeric(df['population'],errors='coerce') #df['population'].astype(float)\n",
    "# print(df)\n",
    "\n",
    "# cleaning the time zone\n",
    "# df['actual_timezone'] = df[\"timezone\"].str.extract(\"/(\\w+)\", expand = True) \n",
    "# df[\"timezone\"]= new[0] \n",
    "# print(df)\n",
    "# print(df[:10])\n",
    "# colors = df['timezone']\n",
    "# kk = colors.str.split(\"\\t\", expand=True,)\n",
    "# #\"@expand=True,).split(\"/\")[2]\n",
    "# # colors[colors.has('America/Los_Angeles')] = 'blue', 'purple'\n",
    "# print(kk['0'])\n",
    "# # colors = {'America/Chicago':'red', 'America/New_York':'blue', 'America/Phoenix':'green', 'America/Los_Angeles':'black'}\n",
    "# # df_la = df.loc[df['timezone'] == 'America/Los_Angeles']\n",
    "# # df_chi = df.loc[df['timezone'] == 'America/Chicago']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original example from the book.\n",
    "It needs tk (and it's been imposible to install it in a mac). So, let's just see a snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn\n",
    "# from matplotlib import pyplot as plt\n",
    "# from nlpia.plots import offline_plotly_scatter_bubble\n",
    "# from nlpia.data.loaders import get_data\n",
    "# df = get_data('cities_us_wordvectors_pca2_meta')\n",
    "# html = offline_plotly_scatter_bubble(\n",
    "#     df.sort_values('population', ascending=False)[:350].copy().sort_values('population'),\n",
    "#     filename='plotly_scatter_bubble.html',\n",
    "#     x='x', y='y',\n",
    "#     size_col='population', text_col='name', category_col='timezone',\n",
    "#     xscale=None, yscale=None, # 'log' or None\n",
    "#     layout={}, marker={'sizeref': 3000})\n",
    "# # {'sizemode': 'area', 'sizeref': 3000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a similar example, just plotting the dataframe. \n",
    "It is not as fancy, but we se similar patterns.\n",
    "\n",
    "Notice that there is [an issue](https://github.com/pandas-dev/pandas/issues/32904) in the implementation of the scatter plot and one has to add the size of the dots \"explicitly\"\n",
    "\n",
    "I tried plenty of ways to use the colors, but this dataframe is odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df.sort_values('population', ascending=False)[:350].copy().sort_values('population')\n",
    "# print(df)\n",
    "# use the scatter function\n",
    "# colors = {'D':'red', 'E':'blue', 'F':'green', 'G':'black'}\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter',x='x',y='y', s=df['population']/10000)\n",
    "# df_la.plot(kind='scatter',x='x',y='y', s=df_la['population']/10000, color=['blue']*len(df_la))\n",
    "# df_chi.plot(kind='scatter',x='x',y='y', s=df_chi['population']/10000)\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.spines['left'].set_position('center')\n",
    "# ax.spines['bottom'].set_position('center')\n",
    "# plt.scatter(df['x'], df['y'], s=df['population']*1000, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the number of cores available\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "num_cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "# gensim crude tokenizer that ignores one-letter words and punctuation\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a list of documents as it's iterable\n",
    "# corpus = ['This is the first document ...',\\\n",
    "# 'another document ...']\n",
    "corpus = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "corpus.append(\"Harry is hairy and faster than Jill.\")\n",
    "corpus.append(\"Jill is not as hairy as Harry.\")\n",
    "training_corpus = []\n",
    "for i, text in enumerate(corpus):\n",
    "    tagged_doc = TaggedDocument(simple_preprocess(text), [i])\n",
    "    training_corpus.append(tagged_doc)\n",
    "print(training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the object \n",
    "model = Doc2Vec(size=100,   # dimensions of the vectors\n",
    "                min_count=2, # min frequency for the tokens\n",
    "                workers=num_cores,  \n",
    "                iter=10)   # number of iterations\n",
    "# Compiling the vocabulary \n",
    "model.build_vocab(training_corpus)\n",
    "\n",
    "# training the model\n",
    "model.train(training_corpus, total_examples=model.corpus_count, epochs=model.iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferring a  vector for a new document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.infer_vector(simple_preprocess('Indeed Jill is the fastest'), steps=10)\n",
    "# This is not a static model. It has to be trained (10 iterations  in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
