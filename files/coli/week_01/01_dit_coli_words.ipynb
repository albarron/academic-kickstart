{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick overview on preprocessing\n",
    "\n",
    "These materials are mostly borrowed from Lane et al. (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let us first import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Thomas Jefferson started building Monticello at the age of 26.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple \"tokeniser\" which gets only alphabetical characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'started', 'building', 'Monticello', 'at', 'the', 'age', 'of']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall('[A-Za-z]+', txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides a ``similar'' tool to tokenise but, in general, it is not enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'started', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26.']\n"
     ]
    }
   ],
   "source": [
    "tokens = txt.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Obviously, we can design a better regular expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', ' ', 'Jefferson', ' ', 'started', ' ', 'building', ' ', 'Monticello', ' ', 'at', ' ', 'the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']\n"
     ]
    }
   ],
   "source": [
    "tokens = re.split(r'([-\\s.,;!?])+', txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The community has created multiple libraries for pre-processing, which include options for tokenisation. One of the most popular ones is [NLTK](http://www.nltk.org). \n",
    "\n",
    "Before using it, you should install it. If using pip, you should do: \n",
    "\n",
    "\\$ pip install --user -U nltk\n",
    "\n",
    "\\$ pip install --user -U numpy\n",
    "\n",
    "\n",
    "An now we can import and use one of its tokenisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'started', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer # import one of the many tokenizers available\n",
    "tokenizer = TreebankWordTokenizer()             # invoke it \n",
    "tokens = tokenizer.tokenize(txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see the difference between tokenising with split() and with NLTK's treebank tokeniser on a different sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT USING split()\t\t ['Monticello', \"wasn't\", 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until', '1987.']\nOUTPUT USING TreebankWordTokenizer\t ['Monticello', 'was', \"n't\", 'designated', 'as', 'UNESCO', 'World', 'Heritage', 'Site', 'until', '1987', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "tokens_split = sentence.split()\n",
    "tokens_tree = tokenizer.tokenize(sentence)\n",
    "\n",
    "print(\"OUTPUT USING split()\\t\\t\", tokens_split)\n",
    "print(\"OUTPUT USING TreebankWordTokenizer\\t\", tokens_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "### Casefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monticello wasn't designated as unesco world heritage site until 1987.\n"
     ]
    }
   ],
   "source": [
    "sentence  = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Once again, we can use a regular expression to do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
    "         word)[0][0].strip(\"'\") for word in phrase.lower()\n",
    "         .split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'houses' \t\t-> house\n'Doctor House's calls' \t-> doctor house call\n'stress' \t\t-> stress\n"
     ]
    }
   ],
   "source": [
    "print(\"'houses' \\t\\t->\", stem('houses'))\n",
    "print(\"'Doctor House's calls' \\t->\", stem(\"Doctor House's calls\"))\n",
    "print(\"'stress' \\t\\t->\", stem(\"stress\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we would need to include many more expressions to deal with all cases and exceptions.\n",
    "\n",
    "Instead, once again we can rely on a library. Let's consider the **Porter stemmer**, available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dish', 'washer', 'wash', 'dish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer # Import the stemmer\n",
    "stemmer = PorterStemmer()                  # invoke the stemmer\n",
    "\n",
    "# Notice that we are \"tokenising\" and stemming in one line\n",
    "x = ' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])\n",
    "print(x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "This is a more complex process, compared to stemming. Let us go straight to use a library.\n",
    "In this particular case we are going to use NLTK's WordNet lemmatiser. If it is the first time you use it (or you are in an ephemeral environment!), you should download it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/albarron/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'better' alone \t-> better\n'better' including it's part of speech (adj) \t-> good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer # importing the lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()        # invoking it\n",
    "\n",
    "print(\"'better' alone \\t->\",lemmatizer.lemmatize(\"better\"))\n",
    "print(\"'better' including it's part of speech (adj) \\t->\",lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick overview on representations\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "First, let us see a simple construction, using a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('26.', 1),\n ('Jefferson', 1),\n ('Monticello', 1),\n ('Thomas', 1),\n ('age', 1),\n ('at', 1),\n ('began', 1),\n ('building', 1),\n ('of', 1),\n ('the', 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26. Thomas\"\"\"\n",
    "\n",
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "     sentence_bow[token] = 1\n",
    "sorted(sentence_bow.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option would be using **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1770.</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>He</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Jefferson's</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Pavilion</th>\n",
       "      <th>South</th>\n",
       "      <th>Thomas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1770.</th>\n",
       "      <th>26.</th>\n",
       "      <th>Construction</th>\n",
       "      <th>He</th>\n",
       "      <th>Jefferson</th>\n",
       "      <th>Jefferson's</th>\n",
       "      <th>Monticello</th>\n",
       "      <th>Pavilion</th>\n",
       "      <th>South</th>\n",
       "      <th>Thomas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the corpus\n",
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "# Loading the tokens into a dictionary (notice that we asume that each line is a document)\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
    "         sent.split())\n",
    "\n",
    "# Loading the dictionary contents into a pandas dataframe. \n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "# SEE THE .T, which transposes the matrix for visualisation purposes.\n",
    "\n",
    "\n",
    "df[df.columns[:10]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectors\n",
    "\n",
    "This is our input sentence (and its vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we produce the one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int) # create the |tokens| x |vocabulary size| matrix of zeros \n",
    "for i, word in enumerate(token_sequence):\n",
    "   onehot_vectors[i, vocab.index(word)] = 1  # set one to right dimension to 1\n",
    "\n",
    "print(\"Vocabulary:\\t\", vocab)\n",
    "print(\"Sentence:\\t\", token_sequence)\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us bring pandas into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
