{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using sklearn to build tf-idf matrices\n",
    "\n",
    "Now, rather than implementing everything ourselves, we will use a well-known python library to compute it for us.\n",
    "\n",
    "**I will move this section to the end of the previous notebook. It doesn't belong here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nlpia.data.loaders import get_data\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from pugnlp.stats import Confusion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same small corpus as before\n",
    "docs = [\"The faster Harry got to the store, the faster and faster Harry would get home.\"]\n",
    "docs.append(\"Harry is hairy and faster than Jill.\")\n",
    "docs.append(\"Jill is not as hairy as Harry.\")\n",
    "corpus = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "\"\"\"min_df: ignore terms that have a document frequency \n",
    "strictly lower than the given threshold (aka cut-off).\n",
    "\"\"\"\n",
    "\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "\"\"\"model is a sparse tf-idf matrix (mostly zeros) \n",
    "sklearn does not store zeros to save resources\"\"\"\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert it into a dense matrix in one line!\n",
    "print(\"\\n--\\n\".join(corpus))\n",
    "print(model.todense().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to today's topic...\n",
    "\n",
    "## Thought Exercise\n",
    "\n",
    "Training a topic model with _common sense_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = {}\n",
    "# zip returns an iterator of tuples, where the i-th tuple \n",
    "# contains the i-th element from each of the argument sequences\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(),\\\n",
    "                      np.random.rand(6))))\n",
    "# Random tf-idf vector for our single document\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have \"created\" common-sense weights\n",
    "# Now, we multiply the tf-idf vector by the \n",
    "# \"hand-crafted‚Äù weights (notice the subtractions)\n",
    "topic['petness'] = (.3 * tfidf['cat'] +\\\n",
    "                .3 * tfidf['dog'] +\\\n",
    "                0 * tfidf['apple'] +\\\n",
    "                0 * tfidf['lion'] -\\\n",
    "                .2 * tfidf['NYC'] +\\\n",
    "                .2 * tfidf['love'])\n",
    "topic['animalness'] = (.1 * tfidf['cat'] +\\\n",
    "                .1 * tfidf['dog'] -\\\n",
    "                .1 * tfidf['apple'] +\\\n",
    "                .5 * tfidf['lion'] +\\\n",
    "                .1 * tfidf['NYC'] -\\\n",
    "                .1 * tfidf['love'])\n",
    "topic['cityness'] = ( 0 * tfidf['cat'] -\\\n",
    "                .1 * tfidf['dog'] +\\\n",
    "                .2 * tfidf['apple'] -\\\n",
    "                .1 * tfidf['lion'] +\\\n",
    "                .5 * tfidf['NYC'] +\\\n",
    "                .1 * tfidf['love'])\n",
    "topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing the 6x3 matrix to produce topic weights for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector = {}\n",
    "# word_vector['cat'] = [.3*topic['petness'] +\\\n",
    "#                     .1*topic['animalness'] +\\\n",
    "#                     0*topic['cityness']\n",
    "\n",
    "# word_vector['dog'] = .3*topic['petness'] +\\\n",
    "#                     .1*topic['animalness'] -\\\n",
    "#                     .1*topic['cityness']\n",
    "\n",
    "# word_vector['apple']= 0*topic['petness'] -\\\n",
    "#                     .1*topic['animalness'] +\\\n",
    "#                     .2*topic['cityness']\n",
    "\n",
    "# word_vector['lion'] = 0*topic['petness'] +\\\n",
    "#                     .5*topic['animalness'] -\\\n",
    "#                     .1*topic['cityness']\n",
    "# word_vector['NYC'] = -.2*topic['petness'] +\\\n",
    "#                     .1*topic['animalness'] +\\\n",
    "#                     .5*topic['cityness']\n",
    "# word_vector['love'] = .2*topic['petness'] -\\\n",
    "#                     .1*topic['animalness'] +\\\n",
    "#                     .1*topic['cityness']\n",
    "\n",
    "word_vector['cat'] = [.3*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    0*topic['cityness']]\n",
    "\n",
    "word_vector['dog'] = [.3*topic['petness'],\n",
    "                    .1*topic['animalness'], \n",
    "                    -.1*topic['cityness']]\n",
    "\n",
    "word_vector['apple']= [0*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    .2*topic['cityness']]\n",
    "\n",
    "word_vector['lion'] = [0*topic['petness'],\n",
    "                    .5*topic['animalness'],\n",
    "                    -.1*topic['cityness']]\n",
    "word_vector['NYC'] = [-.2*topic['petness'],\n",
    "                    .1*topic['animalness'],\n",
    "                    .5*topic['cityness']]\n",
    "word_vector['love'] = [.2*topic['petness'],\n",
    "                    -.1*topic['animalness'],\n",
    "                    .1*topic['cityness']]\n",
    "word_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Linear Discriminant Analysis classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a labeled corpus: spam\n",
    "sms = get_data('sms-spam')\n",
    "print(sms[:-10])\n",
    "\n",
    "# Just setting up the printing properties\n",
    "pd.options.display.width = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For display purposes: spam instances have a \"!\" added to the label\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in \\\n",
    "         zip(range(len(sms)), sms.spam)]\n",
    "print(index[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'!'*0\n",
    "#'!'*1\n",
    "#'!'*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pandas df, using the data and the new index\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)\n",
    "print(sms)\n",
    "# len(sms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: what am I getting with this sum?\n",
    "sms.spam.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorising the corpus\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=sms.text).toarray()\n",
    "# QUESTION: what is the number on the right?\n",
    "tfidf_docs.shape\n",
    "tfidf_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "We have \n",
    "* 4837 messages\n",
    "* 638 positive instances\n",
    "* 9232 types\n",
    "\n",
    "That's too much for a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the LDA\n",
    "\n",
    "We just need the centroids of spam and non-spam, so we implement it \n",
    "\n",
    "(keep in mind that sklearn has an [LDA](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A mask (or \"filter\") to select only spam messages\n",
    "mask = sms.spam.astype(bool).values\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the spam centroid\n",
    "spam_centroid = tfidf_docs[mask].mean(axis=0)\n",
    "# axis=0 tells numpy to compute the mean for each column independently\n",
    "print(spam_centroid.round(2))\n",
    "len(spam_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the ham centroid\n",
    "ham_centroid = tfidf_docs[~mask].mean(axis=0)\n",
    "print(ham_centroid.round(2))\n",
    "len(ham_centroid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_centroid - ham_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the centroid difference: \"the line between spam and ham\"\n",
    "spamminess_score = tfidf_docs.dot(spam_centroid - ham_centroid)\n",
    "print(spamminess_score.round(2))\n",
    "len(spamminess_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not just subtracting. We computed the dot product!\n",
    "\n",
    "**spamminess_score** is $dis(centroid_{(spam)}, centroid_{(ham)})$\n",
    "\n",
    "We compute it by projecting each TF-IDF vector onto that line between the centroids using the dot product (those were indeed 4,837 dot products computed at once!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning into \"probabilities\" and predictions\n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_score.reshape(-1,1))\n",
    "sms['lda_predict'] = (sms.lda_score > .5).astype(int)\n",
    "\n",
    "sms['spam lda_predict lda_score'.split()].round(2).head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is accuracy of the model?\n",
    "(1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a confusion matrix\n",
    "Confusion(sms['spam lda_predict'.split()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
