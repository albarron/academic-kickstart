{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick overview on preprocessing\n",
    "\n",
    "These materials are mostly borrowed from [Lane et al. (2019)](https://www.manning.com/books/natural-language-processing-in-action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, let us first import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Thomas Jefferson started building Monticello at the age of 26.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple \"tokeniser\", which captures alphabetical characters only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.findall('[A-Za-z]+', txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides a \"similar\" tool to tokenise but, in general, it is not enough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = txt.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Obviously, we can design a better regular expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = re.split(r'([-\\s.,;!?])+', txt)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987\"\n",
    "tokens = re.split(r'([-\\s.,;!?])+', text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "The community has created multiple libraries for pre-processing, which include options for tokenisation. Two of the most popular ones are\n",
    "\n",
    "* [NLTK](http://www.nltk.org). \n",
    "* [Spacy](https://spacy.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it is the first time you use them (and this is mostly true if you are using an ephimerous platform, such as colab), you should install it. \n",
    "\n",
    "You can do so with [pip](https://pip.pypa.io/en/stable/): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install --user -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working from the terminal, in local, you might have to do\n",
    "\n",
    "```\n",
    "$ pip install --user -U spacy\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An now we can import and use one of its tokenisers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the library\n",
    "import spacy\n",
    "\n",
    "# downloading the model\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(txt)\n",
    "print([token.text for token in doc])\n",
    "\n",
    "# from nltk.tokenize import TreebankWordTokenizer # import one of the many tokenizers available\n",
    "# tokenizer = TreebankWordTokenizer()             # invoke it \n",
    "# tokens = tokenizer.tokenize(txt)\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, see the difference between tokenising with split() and with spacy's web tokeniser on a different sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\n",
    "tokens_split = sentence.split()\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"OUTPUT USING split()\\t\", tokens_split)\n",
    "print(\"OUTPUT USING spacy\\t\", [token.text for token in  doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "### Casefolding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence  = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Once again, we can use a regular expression to do stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(phrase):\n",
    "    return ' '.join([re.findall('^(.*ss|.*?)(s)?$',\n",
    "         word)[0][0].strip(\"'\") for word in phrase.lower()\n",
    "         .split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"'houses' \\t\\t->\", stem('houses'))\n",
    "print(\"'Doctor House's calls' \\t->\", stem(\"Doctor House's calls\"))\n",
    "print(\"'stress' \\t\\t->\", stem(\"stress\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we would need to include many more expressions to deal with all cases and exceptions.\n",
    "\n",
    "Instead, once again we can rely on a library. Let's consider the **Porter stemmer**, available in NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing NLTK (and its dependency: numpy)#\n",
    "! pip install --user -U nltk\n",
    "! pip install --user -U numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer # Import the stemmer\n",
    "stemmer = PorterStemmer()                  # invoke the stemmer\n",
    "\n",
    "# Notice that we are \"tokenising\", stemming, and concatenating in one line!\n",
    "# Notice that these operations \"appear\" inverted in the code (let us have a look together) \n",
    "x = ' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])\n",
    "print(x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "\n",
    "This is a more complex process, compared to stemming. Let us go straight to use a library.\n",
    "In this particular case we are going to use NLTK's WordNet lemmatiser. If it is the first time you use it (or you are in an ephemeral environment!), you should download it as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLTK alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer # importing the lemmatiser\n",
    "lemmatizer = WordNetLemmatizer()        # invoking it\n",
    "\n",
    "print(\"'better' alone \\t->\",lemmatizer.lemmatize(\"better\"))\n",
    "print(\"'better' incl. it's POS (adj) \\t->\",lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spacy alternative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"better\")\n",
    "print([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick overview on representations\n",
    "\n",
    "### Bag of Words (BoW)\n",
    "\n",
    "First, let us see a simple construction, using a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26. Thomas\"\"\"\n",
    "\n",
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "     sentence_bow[token] = 1\n",
    "sorted(sentence_bow.items())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the slides**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option would be using **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Loading the corpus\n",
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "# Loading the tokens into a dictionary (notice that we asume that each line is a document)\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
    "         sent.split())\n",
    "\n",
    "# Loading the dictionary contents into a pandas dataframe. \n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "# SEE THE .T, which transposes the matrix for visualisation purposes.\n",
    "\n",
    "\n",
    "df[df.columns[:10]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot vectors\n",
    "\n",
    "This is our input sentence (and its vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "token_sequence = str.split(sentence)\n",
    "vocab = sorted(set(token_sequence))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we produce the one-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "onehot_vectors = np.zeros((num_tokens, vocab_size), int) # create the |tokens| x |vocabulary size| matrix of zeros \n",
    "print(token_sequence)\n",
    "print(onehot_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(token_sequence):\n",
    "   onehot_vectors[i, vocab.index(word)] = 1  # set one to right dimension to 1\n",
    "\n",
    "print(\"Vocabulary:\\t\", vocab)\n",
    "print(\"Sentence:\\t\", token_sequence)\n",
    "onehot_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us bring pandas into the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(onehot_vectors, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of the notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
