{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the tokens from a corpus\n",
    "\n",
    "This jupyter notebook is intended for lesson 2/3 of DIT's Natural Language Processing course\n",
    "\n",
    "We are going to setup a toy corpus and compute some _similarities_.\n",
    "\n",
    "The libraries to be used today are:\n",
    "\n",
    "* [numpy](https://numpy.org/)\n",
    "* [pandas](https://pandas.pydata.org/)\n",
    "* [nltk](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "Since we are going to use non-standard libraries, we need to set them up --if working on an ephemeral environment (e.g., colab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the stopwords collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the preprocessing _pipeline_:\n",
    "\n",
    "1. Tokenisation\n",
    "2. Stemmming\n",
    "3. Stopwording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking the necessary objects\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tiny test\n",
    "tokenizer.tokenize(\"The input text.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both tokenisation and stemming\n",
    "text = \"\"\"Perseverance (nicknamed Percy) is a car-sized Mars \n",
    "rover designed to explore the crater Jezero on Mars as part \n",
    "of NASA's Mars 2020 mission.\"\"\"\n",
    "\n",
    "print([stemmer.stem(w) for w in tokenizer.tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first time you use the stopwords, you have to download them!\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a stopword?\n",
    "\n",
    "According to [the Wikipedia](https://en.wikipedia.org/wiki/Stop_word): \"the words in a stop list (or stoplist or negative dictionary) which are **filtered out** (i.e. stopped) before or after processing of natural language data (text) because they are insignificant.\"\n",
    "\n",
    "For some search engines, these are **some of the most common, short function words,** such as the, is, at, which, and on. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as \"The Who\", \"The The\", or \"Take That\". \n",
    "\n",
    "**Q: Can I create a list of stopwords on the fly?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up the _corpus_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "\n",
    "sentence = sentences.lower()\n",
    "sentence      # what is the diff wrt print()?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bag of words representation\n",
    "\n",
    "Let us compute the BoW representation for our toy corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the corpus into a dictionary\n",
    "corpus ={}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    sentence = sent.lower()                 # Case folding\n",
    "    tokens = tokenizer.tokenize(sentence)   # Tokenisation \n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in\n",
    "         stems)\n",
    "\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data into a pandas dataframe\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "\n",
    "# df[df.columns[:10]]\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Computing the dot product\n",
    "\n",
    "\"The sum of the products of the corresponding entries of two sequences of numbers\". \n",
    "\n",
    "Let us go and have a look at the [Wikipedia](https://en.wikipedia.org/wiki/Dot_product).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([2, 4, 6])\n",
    "\n",
    "\n",
    "# The long way\n",
    "sum_dot = 0\n",
    "\n",
    "for i in range(len(v1)):\n",
    "    sum_dot += v1[i] * v2[i]\n",
    "    print(\"result at iteration {}: {}\".format(i, sum_dot))\n",
    "print(\"Result:\", sum_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The smart way (we are \"vectorising\")\n",
    "dot = (v1 * v2).sum()\n",
    "print(dot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numpy way\n",
    "v1.dot(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product can be used to measure the overlap between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We first need to compute the transpose of the matrix \n",
    "# because I need column vectors\n",
    "\n",
    "df = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How can I print it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sent0.dot(df.sent3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where do these numbers come from?\n",
    "print(sentences)\n",
    "[(k, v) for (k, v) in (df.sent0 & df.sent3).items() if v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is your first **vector space model**!\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
