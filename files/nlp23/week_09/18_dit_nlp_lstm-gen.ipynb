{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text with LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/albarron/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run only the first time\n",
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# In previous versions of Keras, the import was it was...\n",
    "# from keras.optimizers import RMSprop\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to [Project Gutenberg](https://www.gutenberg.org) if you want more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 375542 total chars: 50\n",
      "[the tragedie of julius caesar by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter flauius, murellus, and certaine commoners ouer the stage.\n",
      "\n",
      "  flauius. hence: home you idle creatures, get you home:\n",
      "is this a holiday? what, know you not\n",
      "(being mechanicall) you ought not walke\n",
      "vpon a labouring day, without the signe\n",
      "of your profession? speake, what trade art thou?\n",
      "  car. why sir, a carpenter\n",
      "\n",
      "   mur. where is thy leather apron, and thy rule?\n",
      "what dost thou with thy best apparrell on\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "for txt in gutenberg.fileids():\n",
    "    if 'shakespeare' in txt:\n",
    "        text += gutenberg.raw(txt).lower()\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# dictionary from character to index\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# distionary from index to character\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "print('corpus length: {} total chars: {}'.format(len(text), len(chars)))\n",
    "\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective.** Predicting the character after having seen 40 characters\n",
    "\n",
    "**Trick.** Adding redundancy to the training collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of sequences: 125168\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "# Notice: no tokenisation; no sentence splitting; no linebreak elimination\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Total number of sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[the tragedie of julius caesar by willia  -->  m\n",
      "==================================================\n",
      "5\n",
      "f julius caesar by william shakespeare 1  -->  5\n",
      "==================================================\n",
      "2555\n",
      "biect of my story:\n",
      "i cannot tell, what y  -->  o\n",
      "==================================================\n",
      "10000\n",
      "d this, metellus\n",
      "cymber\n",
      "\n",
      "   brut. they a  -->  r\n",
      "==================================================\n",
      "12365\n",
      "hould not know you brutus. deare my lord  -->  ,\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# How does an instance look like?\n",
    "\n",
    "idx = [0, 5, 2555, 10000, 12365]\n",
    "\n",
    "for i in idx:\n",
    "    print(i)\n",
    "    print(sentences[i], \" --> \", next_chars[i])\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Producing the one-hot encoding\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,\n",
    "    input_shape=(maxlen, len(chars))))\n",
    "\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# https://keras.io/api/optimizers/rmsprop/; learning rate default: 0.001\n",
    "# learning_rate used to be lr in previous versions\n",
    "optimizer = RMSprop(learning_rate=0.01)\n",
    "\n",
    "# no more binary cross entropy; no dropout (we \"kind of\" want to overfit here)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "978/978 [==============================] - 62s 62ms/step - loss: 2.3554\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 63s 65ms/step - loss: 1.7113\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 65s 66ms/step - loss: 1.5819\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 67s 68ms/step - loss: 1.5025\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 66s 67ms/step - loss: 1.4644\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 65s 66ms/step - loss: 1.4190\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 67s 69ms/step - loss: 1.4180\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 65s 66ms/step - loss: 1.3992\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 64s 65ms/step - loss: 1.3802\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 65s 67ms/step - loss: 1.3676\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 62s 63ms/step - loss: 1.3567\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 62s 63ms/step - loss: 1.3464\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 70s 71ms/step - loss: 1.3357\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 73s 74ms/step - loss: 1.3279\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 73s 74ms/step - loss: 1.3215\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 75s 77ms/step - loss: 1.3129\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 78s 80ms/step - loss: 1.3083\n",
      "Epoch 6/6\n",
      "978/978 [==============================] - 89s 91ms/step - loss: 1.3002\n",
      "Epoch 1/6\n",
      "978/978 [==============================] - 94s 96ms/step - loss: 1.2929\n",
      "Epoch 2/6\n",
      "978/978 [==============================] - 109s 112ms/step - loss: 1.2908\n",
      "Epoch 3/6\n",
      "978/978 [==============================] - 104s 107ms/step - loss: 1.2850\n",
      "Epoch 4/6\n",
      "978/978 [==============================] - 150s 154ms/step - loss: 1.2802\n",
      "Epoch 5/6\n",
      "978/978 [==============================] - 94s 96ms/step - loss: 1.2739\n",
      "Epoch 6/6\n",
      "468/978 [=============>................] - ETA: 35s - loss: 1.2428"
     ]
    }
   ],
   "source": [
    "# Training \n",
    "epochs = 6\n",
    "batch_size = 128\n",
    "model_structure = model.to_json()\n",
    "\n",
    "with open(\"shakes_lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "for i in range(5):\n",
    "    model.fit(X, y,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs\n",
    "    )\n",
    "    model.save_weights(\"shakes_lstm_weights_{}.h5\".format(i+1))\n",
    "    \n",
    "# It should take about 25 epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Temperature**\n",
    "\n",
    "temperature > 1 : more diverse outcome\n",
    "\n",
    "temperature < 1 : more strict (try to \"copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    \"\"\"Sampler to generate character sequences\"\"\"\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    # produces a number of random outcomes, \n",
    "    # given a probability distribution\n",
    "    # n=1    number of experiments\n",
    "    # preds  sequence of probabilities\n",
    "    # size=1 \n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print()\n",
    "    print('----- diversity:', diversity)\n",
    "    # Getting a random starting text\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    generated = ''\n",
    "    generated += sentence\n",
    "    \n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    for i in range(400):\n",
    "        # one-hot representation\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        \n",
    "        # Producing the prediction\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        \n",
    "        # looking up the next character and adding it \n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        \n",
    "        #updating the seed\n",
    "        sentence = sentence[1:] + next_char\n",
    "        \n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()  # to display it right away\n",
    "    print()\n",
    "    \n",
    "# lower values should look \"more Shakesperean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the network\n",
    "model.compile('rmsprop', \n",
    "              'binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's **70,200** parameters in the LSTM (against **17,550** for the RNN)\n",
    "\n",
    "Back to the slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the network\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the network for future use\n",
    "model_structure = model.to_json()\n",
    "with open(\"lstm_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"lstm_weights1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting\n",
    "sample_1 = \"\"\"I hate that the dismal weather had me down for so long, when\n",
    "will it break! Ugh, when does happiness return? The sun is blinding and\n",
    "the puffy clouds are too thin. I can't wait for the weekend.\"\"\"\n",
    "\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(\n",
    "    test_vec_list, \n",
    "    (len(test_vec_list), maxlen, embedding_dims))\n",
    "model.predict_classes(test_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. Try with shorter/longer contextual sequences\n",
    "2. Build a model that tries to mimic Dante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
